{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imported libraries and Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'utils' from 'c:\\\\Users\\\\utilizador\\\\Desktop\\\\ac-feup\\\\jupyters\\\\hugo\\\\utils.py'>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### imported Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import importlib\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "\n",
    "### Sklearn imported libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "\n",
    "# Pipeline for Oversampling\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "### Imported Scripts\n",
    "import utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "importlib.reload(utils)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Dataset Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dummy(df,columns):\n",
    "    copy= df.copy()\n",
    "\n",
    "    for column in columns:\n",
    "        print(copy.shape)\n",
    "        dummies = pd.get_dummies(copy[column])\n",
    "        copy = copy.drop(column,axis=1)\n",
    "        copy = copy.join(dummies)\n",
    "    \n",
    "    return copy\n",
    "\n",
    "def convert_dates(df):\n",
    "    copy = df.copy()\n",
    "    columns = [\"loan_date\",\"account_creation\",\"birth_number\"]\n",
    "\n",
    "    for column in columns:\n",
    "        copy[column] = copy[column].apply(lambda x: datetime.strptime(x, '%d-%m-%Y').strftime('%Y')).astype(int)\n",
    "\n",
    "    copy[\"age_on_loan\"] = copy[\"loan_date\"] - copy[\"birth_number\"]\n",
    "    copy = copy.drop(columns = [\"loan_date\",\"account_creation\",\"birth_number\"])\n",
    "\n",
    "    copy['card_issued'] = pd.to_numeric(copy[\"card_issued\"].astype(str), errors='coerce').fillna(1).astype(int)\n",
    "\n",
    "    return copy\n",
    "\n",
    "def get_df(test=False, dummies=False, category_encoding=False):\n",
    "    if test:\n",
    "        df = pd.read_csv('../../csvs/loan_united_test.csv', sep=',')\n",
    "    else:\n",
    "        df = pd.read_csv('../../csvs/loan_united_train.csv', sep=',')\n",
    "    \n",
    "    columns = [\"account_frequency\",\"gender\",\"card_type\"]\n",
    "\n",
    "    if dummies:\n",
    "        df = add_dummy(df, columns)\n",
    "    \n",
    "    if category_encoding:\n",
    "        df = utils.normalize_category(df)\n",
    "    \n",
    "    df = convert_dates(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split the data\n",
    "def split_dataset(df,ratio=0.7,debug=False,n_columns = 15):\n",
    "\n",
    "    ### Seperate the precition columns from output\n",
    "    X = df.drop(columns=['loan_success'])\n",
    "    y = df['loan_success']\n",
    "\n",
    "    select = SelectKBest(f_classif, k= n_columns)\n",
    "    X_new = select.fit_transform(X, y)\n",
    "\n",
    "    filter = select.get_support()\n",
    "    features = X.columns\n",
    "\n",
    "    if debug:\n",
    "        print(\"Selected best 3:\")\n",
    "        print(features[filter])\n",
    "        print(X_new) \n",
    "\n",
    "    ### Apply splitting\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_new,y,train_size=ratio,test_size=1-ratio)\n",
    "\n",
    "    return X_train,X_test,y_train,y_test, features[filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_forest():\n",
    "    return RandomForestClassifier(bootstrap = False,\n",
    "                                    max_depth = 80,\n",
    "                                    max_features = 3,\n",
    "                                    min_samples_leaf = 3,\n",
    "                                    min_samples_split = 12,\n",
    "                                    n_estimators = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logistic_regression():\n",
    "    return LogisticRegression(random_state=10,solver='lbfgs',max_iter=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decision_tree():\n",
    "    return DecisionTreeClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_knn():\n",
    "    return KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use *Grid Search Cross Validation* to find the best grid for an algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Uses a grid search to generate random parameters to find the best grid model\n",
    "def getBestSearch(algorithm,grid,debug=True):\n",
    "\n",
    "    # train = pd.read_csv('../../project/banking_data/loanUnitedTrain.csv', sep=',')\n",
    "    train = pd.read_csv('../../csvs/loan_united_train.csv', sep=',')\n",
    "    train = utils.normalize_category(train)\n",
    "\n",
    "    X = train.drop(columns=['loan_success'])\n",
    "    y = train['loan_success']\n",
    "\n",
    "    alg = algorithm()\n",
    "\n",
    "    grid_search = GridSearchCV(estimator = alg, param_grid = grid, cv = 2, n_jobs = -1, verbose = 2)\n",
    "\n",
    "    model = grid_search.fit(X,y)\n",
    "\n",
    "    if debug:\n",
    "        print('Best Score: ', model.best_score_)\n",
    "        print('Best Params: ', model.best_params_)\n",
    "    \n",
    "    return model.best_score_, model.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Pipeline to apply a sampling and a classification algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sampling (This should work if the ASHDASJDSAKD LIBRARY IS IMPORTED)\n",
    "### TODO: Add undersample before final delivery\n",
    "def build_pipeline(algorithm,oversample=True,undersample=True):\n",
    "\n",
    "    if(oversample):\n",
    "        return Pipeline([\n",
    "            ('sampling',SMOTE(random_state = 20)),\n",
    "            ('classification',algorithm)\n",
    "        ])\n",
    "    else:\n",
    "        return  Pipeline([\n",
    "            ('classification',algorithm)\n",
    "        ])\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Stratified Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_CV(algorithm,n_splits = 3):\n",
    "    # train = pd.read_csv('../../project/banking_data/loanUnitedTrain.csv', sep=',')\n",
    "    train = pd.read_csv('../../csvs/loan_united_train.csv', sep=',')\n",
    "    df = utils.normalize_category(train)\n",
    "    # df = utils.normalization(df,'loan_success')\n",
    "\n",
    "    X = df.drop(columns=['loan_success'])\n",
    "    y = df['loan_success']\n",
    "\n",
    "    select = SelectKBest(f_classif, k=15)\n",
    "    X_new = select.fit_transform(X, y)\n",
    "\n",
    "    algorithm = algorithm()\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, random_state=True, shuffle=True)\n",
    "    skf.get_n_splits(X_new, y)\n",
    "\n",
    "    model_list = []\n",
    "    auc_list = []\n",
    "\n",
    "\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        ### Train the model\n",
    "        model = algorithm.fit(X_train, y_train)\n",
    "\n",
    "        ### Predict the outcome with the test data\n",
    "        y_pred = algorithm.predict_proba(X_test)\n",
    "        y_final = y_pred.transpose()[0]\n",
    "\n",
    "        auc = utils.get_auc(y_test, y_final)\n",
    "        auc_list.append(auc)\n",
    "        model_list.append(model)\n",
    "        print(f\"AUC={auc}\")\n",
    "        \n",
    "    ### Get the best model\n",
    "    best_score = max(auc_list)\n",
    "    best_model = model_list[auc_list.index(best_score)]\n",
    "    \n",
    "    \n",
    "    ### Use the best model to get a prediction\n",
    "    test = pd.read_csv('../../csvs/loan_united_test.csv', sep=',')\n",
    "    test = utils.normalize_category(test)\n",
    "    \n",
    "    X = test.drop(columns=['loan_success'])\n",
    "    y_predicted = best_model.predict_proba(X)\n",
    "    y_final = y_predicted.transpose()[0]\n",
    "    \n",
    "    final_df = pd.DataFrame()\n",
    "    final_df['Id'] = test[\"loan_id\"]\n",
    "    final_df['Predicted'] = y_final\n",
    "    \n",
    "    \n",
    "    if True:\n",
    "        print(f\"Predictions:\\n {final_df}\")\n",
    "    \n",
    "    if True:\n",
    "        final_df.to_csv('CV.csv', index=False)\n",
    "        print(\"Sucessfully stored the predictions in a file named 'CV.csv'\")\n",
    "\n",
    "    ### TODO: display statistics?\n",
    "    avg = sum(auc_list)/len(auc_list)\n",
    "    print(f\"Average AUC = {avg}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_algorithm(algorithm,n_columns=15,debug=False):\n",
    "    ### Getting the dataset\n",
    "    # train = pd.read_csv('../../project/banking_data/loanUnitedTrain.csv', sep=',')\n",
    "    \n",
    "    # train = reverse_dates()\n",
    "    \n",
    "    train = pd.read_csv('../../csvs/loan_united_train.csv', sep=',')\n",
    "    train = utils.normalize_category(train)\n",
    "    \n",
    "    ### Getting a Model from training\n",
    "    \n",
    "    X_train,X_test,y_train,y_test, features = split_dataset(train,n_columns = n_columns)\n",
    "\n",
    "    \n",
    "    pipe = build_pipeline(algorithm())\n",
    "    model = pipe.fit(X_train,y_train)\n",
    "\n",
    "    y_predicted = model.predict_proba(X_test)\n",
    "\n",
    "    y_final = y_predicted.transpose()[0]\n",
    "\n",
    "    if debug:\n",
    "        score = model.score(X_test,y_test)\n",
    "        auc = utils.get_auc(y_test,y_final)\n",
    "        print(f\"Score: {score}\")\n",
    "        print(f\"Auc: {auc}\")\n",
    "    \n",
    "    return model, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_model(model,features,debug=False,write=False):\n",
    "    test = pd.read_csv('../../csvs/loan_united_test.csv', sep=',')\n",
    "    # test = pd.read_csv('../../project/banking_data/loanUnitedTest.csv', sep=',')\n",
    "    test = utils.normalize_category(test)\n",
    "\n",
    "    X = test.drop(columns=['loan_success'])\n",
    "    X = X[features]\n",
    "    # print(X)\n",
    "    # X_new = SelectKBest(f_classif, k=20).fit_transform(X)\n",
    "\n",
    "    y_predicted = model.predict_proba(X)\n",
    "    y_final = y_predicted.transpose()[0]\n",
    "    \n",
    "    final_df = pd.DataFrame()\n",
    "    final_df['Id'] = test[\"loan_id\"]\n",
    "    final_df['Predicted'] = y_final\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Predictions:\\n {final_df}\")\n",
    "    \n",
    "    if write:\n",
    "        final_df.to_csv('out.csv', index=False)\n",
    "        print(\"Sucessfully stored the predictions in a file named 'out.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_algorithm(algorithm,debug = True,write=False):\n",
    "    \n",
    "    if(debug):\n",
    "        print(\"Running the provided algorithm\")\n",
    "    \n",
    "    model, features = training_algorithm(algorithm,debug=debug)\n",
    "    # print(features)\n",
    "    testing_model(model,features,debug=debug,write=write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose your algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Leave the one you want to run uncommented\n",
    "algorithm = get_random_forest\n",
    "# algorithm = get_logistic_regression\n",
    "# algorithm = get_decision_tree\n",
    "# algorithm = get_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Chosen Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the provided algorithm\n",
      "Score: 0.8282828282828283\n",
      "Auc: 0.6370481927710844\n",
      "Predictions:\n",
      "        Id  Predicted\n",
      "0    5895   0.114356\n",
      "1    7122   0.738131\n",
      "2    6173   0.094978\n",
      "3    6142   0.416769\n",
      "4    5358   0.464194\n",
      "5    6095   0.106075\n",
      "6    6878   0.047405\n",
      "7    6554   0.469121\n",
      "8    6793   0.277629\n",
      "9    7286   0.162249\n",
      "10   6076   0.045818\n",
      "11   5134   0.185271\n",
      "12   5419   0.131212\n",
      "13   6255   0.442132\n",
      "14   5656   0.135685\n",
      "15   6934   0.351881\n",
      "16   6028   0.102327\n",
      "17   6490   0.239786\n",
      "18   6415   0.440972\n",
      "19   7087   0.052100\n",
      "20   5420   0.100003\n",
      "21   5977   0.062076\n",
      "22   6824   0.486648\n",
      "23   5207   0.199961\n",
      "24   7115   0.134744\n",
      "25   7250   0.010317\n",
      "26   6010   0.095024\n",
      "27   6088   0.534584\n",
      "28   5682   0.067758\n",
      "29   7201   0.040806\n",
      "..    ...        ...\n",
      "324  5698   0.490873\n",
      "325  5169   0.052117\n",
      "326  7294   0.083641\n",
      "327  5318   0.368653\n",
      "328  5368   0.174990\n",
      "329  6923   0.066996\n",
      "330  5463   0.194523\n",
      "331  5265   0.188591\n",
      "332  6321   0.112409\n",
      "333  5226   0.415569\n",
      "334  6868   0.106837\n",
      "335  4967   0.769056\n",
      "336  5293   0.146536\n",
      "337  5865   0.172556\n",
      "338  5841   0.085114\n",
      "339  5526   0.025829\n",
      "340  6852   0.071186\n",
      "341  6469   0.105698\n",
      "342  6168   0.018095\n",
      "343  7292   0.024607\n",
      "344  5036   0.360304\n",
      "345  5644   0.303947\n",
      "346  6856   0.144894\n",
      "347  5428   0.431481\n",
      "348  5027   0.065095\n",
      "349  4989   0.638543\n",
      "350  5221   0.071836\n",
      "351  6402   0.078093\n",
      "352  5346   0.177672\n",
      "353  6748   0.123547\n",
      "\n",
      "[354 rows x 2 columns]\n",
      "Sucessfully stored the predictions in a file named 'out.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilizador\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:439: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
     ]
    }
   ],
   "source": [
    "run_algorithm(algorithm,debug=True,write=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Final Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC=0.7227393617021276\n",
      "AUC=0.7666666666666666\n",
      "AUC=0.7333333333333334\n",
      "Predictions:\n",
      "        Id  Predicted\n",
      "0    5895   0.033163\n",
      "1    7122   0.427416\n",
      "2    6173   0.058175\n",
      "3    6142   0.102339\n",
      "4    5358   0.493692\n",
      "5    6095   0.084053\n",
      "6    6878   0.075648\n",
      "7    6554   0.140303\n",
      "8    6793   0.120538\n",
      "9    7286   0.145471\n",
      "10   6076   0.074315\n",
      "11   5134   0.127673\n",
      "12   5419   0.402554\n",
      "13   6255   0.202907\n",
      "14   5656   0.151114\n",
      "15   6934   0.353400\n",
      "16   6028   0.130194\n",
      "17   6490   0.117759\n",
      "18   6415   0.222100\n",
      "19   7087   0.054107\n",
      "20   5420   0.109486\n",
      "21   5977   0.084928\n",
      "22   6824   0.245381\n",
      "23   5207   0.169652\n",
      "24   7115   0.233245\n",
      "25   7250   0.022611\n",
      "26   6010   0.130106\n",
      "27   6088   0.203942\n",
      "28   5682   0.041326\n",
      "29   7201   0.037770\n",
      "..    ...        ...\n",
      "324  5698   0.218632\n",
      "325  5169   0.104790\n",
      "326  7294   0.064533\n",
      "327  5318   0.146638\n",
      "328  5368   0.141914\n",
      "329  6923   0.124275\n",
      "330  5463   0.167760\n",
      "331  5265   0.044610\n",
      "332  6321   0.037867\n",
      "333  5226   0.135229\n",
      "334  6868   0.069110\n",
      "335  4967   0.448500\n",
      "336  5293   0.051765\n",
      "337  5865   0.064606\n",
      "338  5841   0.040157\n",
      "339  5526   0.060181\n",
      "340  6852   0.070474\n",
      "341  6469   0.029199\n",
      "342  6168   0.222059\n",
      "343  7292   0.075292\n",
      "344  5036   0.084825\n",
      "345  5644   0.464407\n",
      "346  6856   0.143227\n",
      "347  5428   0.164650\n",
      "348  5027   0.039952\n",
      "349  4989   0.290788\n",
      "350  5221   0.034429\n",
      "351  6402   0.069944\n",
      "352  5346   0.076501\n",
      "353  6748   0.134962\n",
      "\n",
      "[354 rows x 2 columns]\n",
      "Sucessfully stored the predictions in a file named 'CV.csv'\n",
      "Average AUC = 0.7409131205673759\n"
     ]
    }
   ],
   "source": [
    "final_CV(algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the best grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'bootstrap': [True,False],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "### Uncomment to run (WARNING: Takes like 5 minutes)\n",
    "# getBestSearch(get_random_forest,param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date(df,column):\n",
    "    copy = df.copy()\n",
    "    date = copy[column]\n",
    "    \n",
    "    copy[column] =  date.apply(lambda x: datetime.datetime.strptime(x, '%d-%m-%Y').strftime('%Y%m%d'))\n",
    "\n",
    "    return copy\n",
    "\n",
    "import datetime\n",
    "\n",
    "def reverse_dates():\n",
    "    \n",
    "    train = pd.read_csv('../../csvs/loan_united_train.csv', sep=',')\n",
    "    columns = ['loan_date','account_creation']\n",
    "    df = train.copy()\n",
    "\n",
    "    for column in columns:\n",
    "        df = convert_date(df,column)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans(n_clusters=3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from pprint import pprint\n",
    "\n",
    "train = pd.read_csv('../../csvs/loan_united_train.csv', sep=',')\n",
    "df = utils.normalize_category(train)\n",
    "# df = utils.normalization(df,'loan_success')\n",
    "\n",
    "X = df.drop(columns=['loan_success'])\n",
    "\n",
    "kmeans_model = KMeans(n_clusters=3).fit(X)\n",
    "# labels = kmeans_model.labels_\n",
    "# metrics.silhouette_score(X, labels, metric='euclidean')\n",
    "\n",
    "pprint(kmeans_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dummy(df,columns):\n",
    "    copy= df.copy()\n",
    "\n",
    "    for column in columns:\n",
    "        print(copy.shape)\n",
    "        dummies = pd.get_dummies(copy[column])\n",
    "        copy = copy.drop(column,axis=1)\n",
    "        copy = copy.join(dummies)\n",
    "    \n",
    "    return copy\n",
    "\n",
    "def df_dummies(test=False):\n",
    "    if test:\n",
    "        df = pd.read_csv('../../csvs/loan_united_test.csv', sep=',')\n",
    "    else:\n",
    "        df = pd.read_csv('../../csvs/loan_united_train.csv', sep=',')\n",
    "\n",
    "    columns = [\"account_frequency\",\"gender\",\"card_type\"]\n",
    "    return add_dummy(df,columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cconverts dates to numerical values\n",
    "def convert_year(df):\n",
    "\n",
    "    copy = df.copy()\n",
    "    columns = [\"loan_date\",\"account_creation\",\"birth_number\"]\n",
    "\n",
    "    for column in columns:\n",
    "        copy[column] = copy[column].apply(lambda x: datetime.strptime(x, '%d-%m-%Y').strftime('%Y')).astype(int)\n",
    "\n",
    "    copy[\"age_on_loan\"] = copy[\"loan_date\"] - copy[\"birth_number\"]\n",
    "    copy = copy.drop(columns = [\"loan_date\",\"account_creation\",\"birth_number\"])\n",
    "\n",
    "    return copy\n",
    "\n",
    "### Replace the card issed column : 0-> no issue 1 -> issue\n",
    "def convert_date_issues(df):\n",
    "    copy = df.copy()\n",
    "    copy['card_issued'] = pd.to_numeric(copy[\"card_issued\"].astype(str), errors='coerce').fillna(1).astype(int)\n",
    "\n",
    "    return copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'datetime' has no attribute 'strptime'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-7580481bea64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../../csvs/loan_united_train.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_year\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mconvert_date_issues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-91-4e6ee4a69c1a>\u001b[0m in \u001b[0;36mconvert_year\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mcopy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'%d-%m-%Y'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%Y'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mcopy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"age_on_loan\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"loan_date\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"birth_number\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3192\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3193\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3194\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3196\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/src\\inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-91-4e6ee4a69c1a>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mcopy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'%d-%m-%Y'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%Y'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mcopy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"age_on_loan\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"loan_date\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"birth_number\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'datetime' has no attribute 'strptime'"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('../../csvs/loan_united_train.csv', sep=',')\n",
    "\n",
    "train2 = convert_year(train)\n",
    "\n",
    "convert_date_issues(train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the provided algorithm\n",
      "Score: 0.6464646464646465\n",
      "Auc: 0.5603174603174603\n",
      "Predictions:\n",
      "        Id  Predicted\n",
      "0    5895   0.000000\n",
      "1    7122   0.333333\n",
      "2    6173   0.000000\n",
      "3    6142   0.333333\n",
      "4    5358   0.000000\n",
      "5    6095   1.000000\n",
      "6    6878   0.000000\n",
      "7    6554   0.333333\n",
      "8    6793   0.000000\n",
      "9    7286   0.666667\n",
      "10   6076   0.000000\n",
      "11   5134   0.333333\n",
      "12   5419   0.333333\n",
      "13   6255   0.000000\n",
      "14   5656   0.000000\n",
      "15   6934   0.333333\n",
      "16   6028   1.000000\n",
      "17   6490   0.000000\n",
      "18   6415   1.000000\n",
      "19   7087   0.000000\n",
      "20   5420   1.000000\n",
      "21   5977   0.000000\n",
      "22   6824   1.000000\n",
      "23   5207   0.000000\n",
      "24   7115   0.333333\n",
      "25   7250   0.000000\n",
      "26   6010   0.000000\n",
      "27   6088   0.000000\n",
      "28   5682   0.666667\n",
      "29   7201   0.333333\n",
      "..    ...        ...\n",
      "324  5698   1.000000\n",
      "325  5169   0.000000\n",
      "326  7294   0.000000\n",
      "327  5318   0.000000\n",
      "328  5368   0.333333\n",
      "329  6923   0.000000\n",
      "330  5463   0.000000\n",
      "331  5265   1.000000\n",
      "332  6321   0.333333\n",
      "333  5226   1.000000\n",
      "334  6868   0.000000\n",
      "335  4967   1.000000\n",
      "336  5293   0.333333\n",
      "337  5865   1.000000\n",
      "338  5841   0.333333\n",
      "339  5526   0.333333\n",
      "340  6852   0.000000\n",
      "341  6469   0.333333\n",
      "342  6168   0.000000\n",
      "343  7292   0.333333\n",
      "344  5036   0.000000\n",
      "345  5644   0.666667\n",
      "346  6856   0.000000\n",
      "347  5428   0.666667\n",
      "348  5027   0.000000\n",
      "349  4989   0.666667\n",
      "350  5221   0.000000\n",
      "351  6402   0.000000\n",
      "352  5346   0.000000\n",
      "353  6748   0.000000\n",
      "\n",
      "[354 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilizador\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:439: UserWarning: X has feature names, but KNeighborsClassifier was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "train = pd.read_csv('../../csvs/loan_united_train.csv', sep=',')\n",
    "test = pd.read_csv('../../csvs/loan_united_test.csv', sep=',')\n",
    "\n",
    "\n",
    "\n",
    "# iris = load_iris()\n",
    "# cross_val_score(clf, iris.data, iris.target, cv=10)\n",
    "\n",
    "run_algorithm(knn,debug=True,write=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c5c685faadbcbae1551fd6d5683e133251e0ca8e77eadbbc8e83131f99b1b14"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
